{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9bf7f890",
      "metadata": {
        "id": "9bf7f890"
      },
      "source": [
        "# Overview of exercises:\n",
        "1. Exploring with blurring, thresholds, and morphological operations to identify objects on an uneven background\n",
        "2. Using the skills from Exercise #1 to segment cell membranes in an image from the early *Drosophila melanogaster* embryo\n",
        "3. Using the 'analyze particles' functionality in Python to measure sizes and other properties of objects"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Areas where you'll need to add or complete code are marked off by a large commented line surrounded with dashes, as below:"
      ],
      "metadata": {
        "id": "YIwYOjQw44-D"
      },
      "id": "YIwYOjQw44-D"
    },
    {
      "cell_type": "code",
      "source": [
        "#-----This marks that there's something to do here-----"
      ],
      "metadata": {
        "id": "pX6sNjLC4_8h"
      },
      "id": "pX6sNjLC4_8h",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Arguments that you'll need to fill in are denoted by question marks, such as 'cv2.medianBlur(?,?)'"
      ],
      "metadata": {
        "id": "ldoB0aD0-NlP"
      },
      "id": "ldoB0aD0-NlP"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Connecting to Google Drive and installing libraries"
      ],
      "metadata": {
        "id": "jVYd2K_Fg8ik"
      },
      "id": "jVYd2K_Fg8ik"
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is all setup. No need to add or change anything here."
      ],
      "metadata": {
        "id": "XVS-9Lah0ypA"
      },
      "id": "XVS-9Lah0ypA"
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect to Google drive to access required data and images\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "YzrHBMi6uYKP"
      },
      "id": "YzrHBMi6uYKP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "201fa421",
      "metadata": {
        "id": "201fa421"
      },
      "outputs": [],
      "source": [
        "# The first two lines (the two '%pip install' ones) used to be necessary for this to work, but aren't any more.\n",
        "# If you receive an error running this cell, try uncommenting them and running the cell again.\n",
        "# %pip install wheel\n",
        "# %pip install plotly==5.4.0\n",
        "\n",
        "import numpy as np\n",
        "import plotly.express as px     # Useful plotting functions\n",
        "import cv2                      # OpenCV, one of several image processing / computer vision libraries\n",
        "import matplotlib.pyplot as plt # More plotting functions\n",
        "from skimage import measure     # Several more functions for image processing\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56b9a36d",
      "metadata": {
        "id": "56b9a36d"
      },
      "source": [
        "## Exercise set #1 - Introduction to thresholding and morphological operations\n",
        "\n",
        "This exercise set includes:\n",
        "- Applying a blur and a basic threshold to an image to segment the image (in other words, separate foreground from background)\n",
        "- Improving the segmentation by subtracting background\n",
        "- Cleaning up the resulting segmentation with one or more binary morphological operations"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7ef38c4",
      "metadata": {
        "id": "a7ef38c4"
      },
      "source": [
        "### Step one - read in a test image. We'll use a default one from Python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e9e651a",
      "metadata": {
        "scrolled": true,
        "id": "3e9e651a"
      },
      "outputs": [],
      "source": [
        "# Run this cell to display an image. The syntax starting with plt.<something> is useful anytime you need to show output\n",
        "from skimage import data\n",
        "img = data.coins() # Grayscale image of some coins\n",
        "#plt.gcf().set_size_inches(10, 10) # If you want the image to be bigger or smaller, uncomment this line and play with the arguments\n",
        "plt.imshow(img,cmap='gray')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c0a20eb",
      "metadata": {
        "id": "5c0a20eb"
      },
      "source": [
        "### Step two - blur and threshold the image to try to segment out the coins. We'll use Otsu's method to threshold"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are several different ways to blur an image. Two common ones are listed below:\n",
        "- **Gaussian**: Set each pixel to a weighted average of the value of all pixels nearby. The weighting here is accomplished using a Gaussian function such that the pixel itself is weighted highest, with pixels farther away contributing less to the average.\n",
        "- **Median**: Set each pixel to the median of the values of all pixels surrounding it. This is good for removing bright pixels of 'noise' from an image.\n",
        "\n",
        "[Otsu's method](https://en.wikipedia.org/wiki/Otsu%27s_method) is an algorithm for automatically determining a good threshold to binarize an image. It's not always the best threshold for a given application, but using it takes some of the manual guesswork out of thresholding and lets you automate code better."
      ],
      "metadata": {
        "id": "IQ-c5AV2D4_y"
      },
      "id": "IQ-c5AV2D4_y"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5dc44a4",
      "metadata": {
        "id": "a5dc44a4"
      },
      "outputs": [],
      "source": [
        "#-----Blur the image two different ways. Insert values where the '?' are to set the strength of the blur-----\n",
        "gaussBlur = cv2.GaussianBlur(img,(?,?),0) # Leaving the 0 here automatically computes width of the Gaussian, but you can set manually if preferred\n",
        "medBlur = cv2.medianBlur(img,?)\n",
        "\n",
        "# Display, using subplots to compare the blur results side by side. Keep this syntax in mind for the rest of the workshop\n",
        "images = [img, gaussBlur, medBlur]\n",
        "titles = ['Original', 'Gaussian', 'Median']\n",
        "plt.imshow(images[0],'gray',vmin=0,vmax=255), plt.title(titles[0])\n",
        "plt.show()\n",
        "plt.subplot(1,2,1), plt.imshow(images[1],'gray',vmin=0,vmax=255), plt.title(titles[1])\n",
        "plt.subplot(1,2,2), plt.imshow(images[2],'gray',vmin=0,vmax=255), plt.title(titles[2])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9775d27",
      "metadata": {
        "scrolled": false,
        "id": "e9775d27"
      },
      "outputs": [],
      "source": [
        "# Threshold coin image to produce binary image - use Otsu's method to automatically set threshold\n",
        "thresh, imgBin = cv2.threshold(img,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU) # Returns threshold, then mask\n",
        "#-----Now do the same for the Guassian and median-blurred images-----\n",
        "\n",
        "# Here's a more compact syntax for plotting things side by side\n",
        "images = [imgBin, ?, ?] # Put your names for the binarized Gaussian and median-blurred images for the '?' here\n",
        "titles = ['Original', 'Gaussian', 'Median']\n",
        "plt.rcParams['figure.figsize'] = [24, 24] # Adjusts the image display size\n",
        "for i in range(3):\n",
        "    plt.subplot(1,3,i+1),plt.imshow(images[i],'gray',vmin=0,vmax=255)\n",
        "    plt.title(titles[i])\n",
        "    plt.xticks([]),plt.yticks([])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a42b6c0",
      "metadata": {
        "id": "8a42b6c0"
      },
      "source": [
        "### Step three - make better masks using a background subtraction prior to the threshold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c4d04c3",
      "metadata": {
        "id": "9c4d04c3"
      },
      "outputs": [],
      "source": [
        "# Subtract a heavily-blurred version of the image from itself\n",
        "\n",
        "#-----Part 1: apply a strong blur to the original image. This needs to be a large value, but less than the image height/width-----\n",
        "blurry = cv2.GaussianBlur(?,(?,?),0)\n",
        "\n",
        "#-----Part 2: substract that from the original-----\n",
        "cleaned = ???\n",
        "\n",
        "#-----Part 3: display the results. See if the results match your guess (and don't worry if not - take a look at the next step)-----\n",
        "???"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What happened?\n",
        "\n",
        "Most often, image data consists of (unsigned) 8-bit integer pixels, meaning each pixel value is an integer from 2<sup>0</sup>-1 = 0 to 2<sup>8</sup>-1 = 255. Because of how math works in binary, negative numbers in unsigned integers are interpreted as very large numbers instead: any time you exceed the 0-255 range with the uint8 data type, you essentially roll over to the other end of the data range.\n",
        "\n",
        "Because of this, in Python at least, you can get some very surprising results from basic math, like 1 - 2 = 255 or 255 + 1 = 0."
      ],
      "metadata": {
        "id": "TYQ9zDdFIylB"
      },
      "id": "TYQ9zDdFIylB"
    },
    {
      "cell_type": "code",
      "source": [
        "#-----Fixing the uint8 quirk: using a cv2 function for subtraction that automatically clips pixel value to 0 if they go 'negative'-----\n",
        "cleaned8 = cv2.subtract(?, ?, np.zeros(np.size(img))) # Arguments: the original image, the image to subtract, and the output destination\n",
        "#-----Look at the background-subtracted version-----\n",
        "\n",
        "#-----Threshold the background-subtracted version and display the results-----\n"
      ],
      "metadata": {
        "id": "jDgwYvqvI4FW"
      },
      "id": "jDgwYvqvI4FW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "5530dbf2",
      "metadata": {
        "id": "5530dbf2"
      },
      "source": [
        "### Step four - use morphological operations to clean up the mask and display the result"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As was mentioned in the lecture, a variety of operations known as 'morphological operations' can be used to clean up binary images (they also work in a similar manner on grayscale images  – e.g. 8-bit images with pixel values between 0-255 – but that's beyond the scope of this session). If you're interested in the logic underlying these operations, here's a quick reference: https://www.cs.auckland.ac.nz/courses/compsci773s1c/lectures/ImageProcessing-html/topic4.htm\n",
        "\n",
        "From the same website, here's a concise summary of why we might want to do this:\n",
        "\n",
        "<blockquote>Binary images may contain numerous imperfections. In particular, the binary regions produced by simple thresholding are distorted by noise and texture. Morphological image processing pursues the goals of removing these imperfections by accounting for the form and structure of the image."
      ],
      "metadata": {
        "id": "z0P4iXGj0R0b"
      },
      "id": "z0P4iXGj0R0b"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is a simple example showing the Python / OpenCV syntax for some morphological operations and their effects on a fake 'image'. Your job will be to use these operations to clean up any segmentations you make in this workshop that you think need slight improvement."
      ],
      "metadata": {
        "id": "mdbcCe4ljJd5"
      },
      "id": "mdbcCe4ljJd5"
    },
    {
      "cell_type": "code",
      "source": [
        "# SAMPLE MORPHOLOGICAL OPERATIONS (erosion, dilation, opening, closing, gradient) - just run this cell; there's nothing you need to do here\n",
        "\n",
        "# Make a structuring element / kernel and the sample image\n",
        "testKern = np.ones((3,3),np.uint8)\n",
        "\n",
        "testIm = np.zeros((101,101), np.uint8)\n",
        "testIm[20:81,20:81] = 255;\n",
        "testIm[20:41,40:42] = 0;\n",
        "testIm[10:11,10:11] = 255;\n",
        "testIm[20:21,10:11] = 255;\n",
        "testIm[20:23,15:18] = 255;\n",
        "\n",
        "# Simple operations (erode and dilate)\n",
        "erode = cv2.morphologyEx(testIm, cv2.MORPH_ERODE, testKern)\n",
        "dilate = cv2.morphologyEx(testIm, cv2.MORPH_DILATE, testKern)\n",
        "\n",
        "# Compound operations (open and close)\n",
        "open = cv2.morphologyEx(testIm, cv2.MORPH_OPEN, testKern)\n",
        "close = cv2.morphologyEx(testIm, cv2.MORPH_CLOSE, testKern)\n",
        "\n",
        "# Gradient (edge finding)\n",
        "grad = cv2.morphologyEx(testIm, cv2.MORPH_GRADIENT, testKern)\n",
        "\n",
        "# Display results by each other\n",
        "plt.rcParams['figure.figsize'] = [13.5, 9] # May have to adjust these for different screen sizes\n",
        "images = [testIm, erode, dilate,\n",
        "          open, close, grad]\n",
        "titles = ['Original', 'Erosion', 'Dilation',\n",
        "          'Opening', 'Closing', 'Gradient']\n",
        "for i in range(6):\n",
        "    plt.subplot(2,3,i+1),plt.imshow(images[i],'gray',vmin=0,vmax=255)\n",
        "    plt.title(titles[i])\n",
        "    plt.xticks([]),plt.yticks([])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "twMTKSHoi2U_"
      },
      "id": "twMTKSHoi2U_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa3481db",
      "metadata": {
        "id": "fa3481db"
      },
      "outputs": [],
      "source": [
        "#----Create kernels for morphological operations. Make one for closing and one for opening, with different sizes-----\n",
        "openKern = np.ones((?,?),np.uint8)\n",
        "closeKern = np.ones((?,?),np.uint8)\n",
        "\n",
        "#-----Perform combination of operations to improve the segmentation-----\n",
        "#-----Close first, then open-----\n",
        "\n",
        "#-----Display results-----\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step five - use the segmentation to 'mask' the original image\n",
        "\n",
        "Sometimes it's useful (or just a good sanity check) to look only at the pixels corresponding to 'foreground'. To do that, you can use the binary segmentation to keep all foreground pixels and set all others to zero.\n",
        "\n",
        "This is a simple operation - think about what the values are in the segmentation image, and remember that images are just matrices of numbers and you can do pixel-by-pixel operations using two different images. Keep in mind that cv2 uses 255 for foreground and 0 for background, rather than 1 for foreground and 0 for background."
      ],
      "metadata": {
        "id": "uIoyxlmbPYAA"
      },
      "id": "uIoyxlmbPYAA"
    },
    {
      "cell_type": "code",
      "source": [
        "#-----Make the masking array (it requires a slight modification to the segmentation to make the math easier)-----\n",
        "\n",
        "#-----Display results-----\n"
      ],
      "metadata": {
        "id": "Hhij37frPXSx"
      },
      "id": "Hhij37frPXSx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise set #2 - Creating a binary image corresponding to cell outlines in the *Drosophila melanogaster* embryo\n",
        "\n",
        "This exercise set includes:\n",
        "- Using what you learned above about thresholding and morphological operations, find cell outlines in an image.\n",
        "- Repeating the segmentation with a more complicated image from the same embryo."
      ],
      "metadata": {
        "id": "fZj3z4AbILuR"
      },
      "id": "fZj3z4AbILuR"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step one - Read in an image with fairly evenly-sized cells\n",
        "\n",
        "This image has bright cell membranes and dark cell bodies, so it won't work exactly the same way as the coin example above. The main idea is similar, though.\n",
        "\n",
        "There are two file paths included below. Feel free to use either, as they work out roughly the same way ()."
      ],
      "metadata": {
        "id": "a18weNtg7_fe"
      },
      "id": "a18weNtg7_fe"
    },
    {
      "cell_type": "code",
      "source": [
        "# Read in the image - this is copied from the pre-work\n",
        "# fname = '/content/gdrive/MyDrive/QMW2023_imageAnalysisWorkshop/embryo_membranes_t1.tif'\n",
        "fname = '/content/gdrive/MyDrive/QMW2023_imageAnalysisWorkshop/embryo2_membranes_t1.tif'\n",
        "emb1 = cv2.imread(fname, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "#-----Display-----\n"
      ],
      "metadata": {
        "id": "KoLhbrPf7uED"
      },
      "id": "KoLhbrPf7uED",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step two - use the concepts from the previous example to pick out the cell membranes"
      ],
      "metadata": {
        "id": "aYA0nqz9_TmI"
      },
      "id": "aYA0nqz9_TmI"
    },
    {
      "cell_type": "markdown",
      "source": [
        "There's a skeletonizing step after this, so if your membranes end up thicker than in the original image, that's fine."
      ],
      "metadata": {
        "id": "_y0634VDSNQT"
      },
      "id": "_y0634VDSNQT"
    },
    {
      "cell_type": "code",
      "source": [
        "# Rescale original image to use full [0 255] range (nothing to change here; it's fixing something from the image acquisition side)\n",
        "emb1raw = emb1.copy()\n",
        "emb1 = (emb1raw-np.min(emb1raw))/np.max(emb1raw-np.min(emb1raw))*255\n",
        "emb1 = emb1.astype(np.uint8) # Back to uint8 type\n",
        "\n",
        "#-----If you want, subtract background and display-----\n",
        "\n",
        "\n",
        "#-----Blur and display-----\n",
        "\n",
        "\n",
        "#-----Threshold, clean up with morphological operations, and display-----\n",
        "\n"
      ],
      "metadata": {
        "id": "4a_32goBKwLj"
      },
      "id": "4a_32goBKwLj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you want to check the binarization and zoom in on the image, use Plotly Express (see the pre-work for syntax)."
      ],
      "metadata": {
        "id": "n9yASUAmSgNK"
      },
      "id": "n9yASUAmSgNK"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step three - thin the segmented membranes to single-pixel thickness\n",
        "\n",
        "The term for this in image processing is 'skeletonizing': an object (here, the cell membranes) will be shrunk in width while maintaining its connectivity (i.e. number of holes). Note that this is different than erosion, which shrinks objects in all directions and can introduce gaps in them."
      ],
      "metadata": {
        "id": "EsA1Ti19CWea"
      },
      "id": "EsA1Ti19CWea"
    },
    {
      "cell_type": "markdown",
      "source": [
        "cv2 does not have a default skeletonize function and writing one from scratch isn't really the point of this exercise, so we'll use one from a different image processing library, sci-kit image (skimage)."
      ],
      "metadata": {
        "id": "LXcKTB_QHXCv"
      },
      "id": "LXcKTB_QHXCv"
    },
    {
      "cell_type": "code",
      "source": [
        "# Skeletonize the image. It requires an image with pixels of 0 and 1, not 0 and 255, hence the division step\n",
        "from skimage.morphology import skeletonize\n",
        "skelled1 = skeletonize(emb1_bin_closed/255)\n",
        "\n",
        "#-----Display. This may show a few different colors depending on color map; if so, that's just a visual quirk-----\n"
      ],
      "metadata": {
        "id": "-W7WZdOEIsbm"
      },
      "id": "-W7WZdOEIsbm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step four - try the same steps on a more complicated image\n",
        "\n",
        "The following image is from the same movie of a developing embryo, taken a bit later. Segmentation will be harder for this one - see if you can guess why from looking at the image.\n",
        "\n",
        "You'll need to adjust parameters in blurs, background subtractions, morphological operations, etc.\n",
        "\n",
        "It's not really possible to get a perfect segmentation, but see how well you can do."
      ],
      "metadata": {
        "id": "ZP4donXZZOb2"
      },
      "id": "ZP4donXZZOb2"
    },
    {
      "cell_type": "code",
      "source": [
        "fname = '/content/gdrive/MyDrive/QMW2023_imageAnalysisWorkshop/embryo_membranes_t33.tif'\n",
        "emb2 = cv2.imread(fname, cv2.IMREAD_GRAYSCALE)"
      ],
      "metadata": {
        "id": "8P8_GKztbMOn"
      },
      "id": "8P8_GKztbMOn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's some code to overlay the skeletonized output on the original image for comparison. You'll need to update the variable names to match yours (emb2 here is the raw data; skelled2 is the skeletonized membrame image).\n",
        "\n",
        "This makes an RGB image with the skeleton in the red channel and the raw data split across all three channels (so it shows as gray). The exact details aren't that important."
      ],
      "metadata": {
        "id": "H4GFK0oRVJIz"
      },
      "id": "H4GFK0oRVJIz"
    },
    {
      "cell_type": "code",
      "source": [
        "emb2scale = emb2*5.5 # Make it brighter for display purposes\n",
        "emb2scale = emb2scale.astype(np.uint8) # Back to uint8\n",
        "merged = cv2.merge([cv2.add(emb2scale,255*skelled2.astype(np.uint8)), emb2scale, emb2scale]) # Gray data, red skeleton\n",
        "plt.imshow(merged)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EyyeKztLic7j"
      },
      "id": "EyyeKztLic7j",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "796acbc0",
      "metadata": {
        "id": "796acbc0"
      },
      "source": [
        "## Exercise set #3 - Identifying, segmenting, and measuring sizes of objects/cells using 'regionprops'\n",
        "\n",
        "This exercise set includes:\n",
        "- Associating a label to each individual object ('connected-components labeling')\n",
        "- Measuring properties from each object, as well as number of objects (e.g. number of coins in the first exercise or number of cells in the field of view in the second)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part one: getting familiar with regionprops using the coin example\n",
        "\n",
        "Let's start with the coin example. Here, counting by eye would be perfectly reasonable, but imagine you have something like this for tens of images and want to get a count for each. Plus, we'll measure things like size.\n",
        "\n",
        "The measurement technique demonstrated here starts from a binary image, so use the best version of the segmentation you got for the coins in Exercise 1."
      ],
      "metadata": {
        "id": "LkZtRsmEfC_n"
      },
      "id": "LkZtRsmEfC_n"
    },
    {
      "cell_type": "code",
      "source": [
        "#-----Enter the name of your best segmentation for the coins here-----\n",
        "coinImg = ?\n",
        "\n",
        "# Object labels and counts\n",
        "labelarray = measure.label(coinImg) # Associates a different numerical label to each blob of foreground pixels\n",
        "plt.imshow(labelarray) # Not super informative, but good to check you don't have two merged objects\n",
        "plt.show()\n",
        "\n",
        "# More detailed data, using a fancier print function to make a rough table\n",
        "print('There are %d coins.' % np.max(labelarray))\n",
        "props = measure.regionprops(labelarray) # Lots of properties, stored in a (number of coins)-element list\n",
        "print(\"{:<15} {:<15} {:<15} {:<15} {:<15} {:<10}\".format('Coin','Size (pixels)','Area (pixels^2)',\n",
        "                                                         'Centroid x','Centroid y','Aspect Ratio'))\n",
        "for prop in props:\n",
        "    cent = np.round(prop.centroid,1)\n",
        "    ar = prop.major_axis_length/prop.minor_axis_length\n",
        "    print(\"{:<15} {:<15} {:<15.1f} {:<15.1f} {:<15.1f} {:<10.3f}\".format(prop.label, prop.area, prop.area, cent[0], cent[1], ar))"
      ],
      "metadata": {
        "id": "0aRlBi9WfVTe"
      },
      "id": "0aRlBi9WfVTe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part two: measuring cell areas from a membrane label image\n",
        "\n",
        "That wasn't the most interesting example, but now that you know the process, you can start getting useful data from segmented images of biological systems. Let's take another look at the *Drosophila* embryo image. We'll use the early image with relatively uniform cell sizes here, but try the more complicated ones if you'd like.\n",
        "\n",
        "The coin example is straightforward, since you had a binary image with bright regions corresponding to the objects of interest. The cell membrane image is different, though: we care about the *cells*, not their membranes, so you'll have to figure out a way to use the membranes to measure the cells. Think back to what you considered foreground and what you considered background when thresholding earlier."
      ],
      "metadata": {
        "id": "PLwp4kehhUfD"
      },
      "id": "PLwp4kehhUfD"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a11c061",
      "metadata": {
        "id": "6a11c061"
      },
      "outputs": [],
      "source": [
        "#-----Get the skeletonized output into a useful form. Note that the skeletonized image is a Boolean, not an integer, and fix this accordingly\n",
        "\n",
        "# Object labels and counts\n",
        "labelarray = measure.label(embImg,connectivity=1) # The connectivity=1 part tells it to not consider diagonal neighbors connected\n",
        "dx = 0.1312 # µm per pixel for this image. Needed to get cell areas in physical units rather than pixel units\n",
        "\n",
        "# Display values as above. Remember to convert area from pixel units to µm^2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sometimes it's more useful to think about mean values than having 100+ sets of values. So let's get the mean and make some histograms to get a better idea of what the cells look like."
      ],
      "metadata": {
        "id": "1DAmKZxbhiv8"
      },
      "id": "1DAmKZxbhiv8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "436ddb25",
      "metadata": {
        "id": "436ddb25"
      },
      "outputs": [],
      "source": [
        "# More detailed data, with some summary statistics of size (that table is unwieldy to read)\n",
        "props = measure.regionprops(labelarray) # Lots of properties, stored in a (number of cell)-element list\n",
        "\n",
        "#-----Generate a list of sizes of every object in the embryo segmentation. Do the same for aspect ratios-----\n",
        "\n",
        "\n",
        "#-----Calculate and print the average cell size-----\n",
        "\n",
        "\n",
        "#-----Slight filtering to remove the larger background regions and at least one of the merged cells-----\n",
        "cleansizes = sizes[(sizes < ?) & (ars < ?)] # Look at the table of areas from above, alongside the image of the embryo cells, and estimate a rough maximum size for actual cells. Same for aspect ratio\n",
        "cleanars = ars[(sizes < ?) & (ars < ?)] # Filter the aspect ratios the same way\n",
        "\n",
        "#-----Calculate and print the average cell size after filtering out bad values-----\n",
        "\n",
        "\n",
        "#-----Plot histograms for cell areas and cell aspect ratios, before and after filtering out bad values-----\n",
        "# If you haven't seen the syntax for histograms before, look up 'matplotlib hist function'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60a623c4",
      "metadata": {
        "id": "60a623c4"
      },
      "source": [
        "## If you've finished all of the examples...\n",
        "Good job! Try to go through the membane segmentation steps and improve your segmentation. Think about what steps you can change (e.g. kernel sizes or morphological operations). We've also included a few other images to try segmenting in the same folder as the membrane images from Exercise 2. If you want a challenge that involves thinking about blur radii, threshold levels, and edge detection, see if you can get a binary image of the coins that picks out the person on the top right coin.\n",
        "\n",
        "We've also added some images of nuclei from the *Drosophila* embryo. Try to segment those - it should be somewhat similar to the coin example, but with actual biological data.\n",
        "\n",
        "If you're happy with the segmentations, below is a brief description of FIJI, a great free tool for visualizing and analyzing images in biology. While this workshop is mainly Python-based, FIJI is such a useful and versatile package that it's worth exploring if you have extra time. Much of the image analysis work done in our lab involves FIJI for at least part of the process."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FIJI\n",
        "\n",
        "[FIJI](https://imagej.net/software/fiji/) (***F***IJI ***I***s ***J***ust ***I***mageJ), and its earlier incarnation, ImageJ, are excellent, free, and open-source tools for viewing images and performing a huge variety of processing/analysis operations on them.\n",
        "\n",
        "FIJI comes with a fairly comprehensive set of image processing tools, but one nice feature of FIJI is the multitude of *plugins* available to provide even more functions. One of these, called MorphoLibJ, is a great resource for image segmentation.\n",
        "\n",
        "## Installing FIJI and exploring a little\n",
        "\n",
        "To install FIJI, please follow the [installation instructions](https://imagej.net/software/fiji/downloads). Everything you need to get started should be included in the download and installation instructions on this page.\n",
        "\n",
        "* Download any of the other images in the 'QMW2023_imageAnalysisWorkshop' folder.\n",
        "* Open the FIJI application. You should see a menu bar somewhere on your screen. FIJI is somewhat overwhelming and hard to navigate at first (the dropdown menus can have dropdown menus that sometimes have dropdown menus, and so on...), but take a look around too see what options there are.\n",
        "* Locate the image file you downloaded. Drag and drop it onto the FIJI bar. Alternatively, Go to **File > Open...** to select your image file from the navigator that opens.\n",
        "* You should now see the image with sliders at the bottom if the image has three or more dimensions. For images, there are generally five possible dimensions:\n",
        "  * Width\n",
        "  * Height\n",
        "  * Depth (z)\n",
        "  * Time (t)\n",
        "  * Color / channel (c) - for example, an image with a nucleus channel and a membrane channel will have two colors.\n",
        "    * Note that files in RGB format are only read as one channel. You can get back to three channels, one per color, using **Image > Color > Split Channels** and **Image > Color > Merge Channels**.\n",
        "* Play around with the sliders and see how the image changes. If you opened a dataset with a time dimension, click the small triangle next to the bar at the bottom (or hit the '\\\\' key) to start or stop the movie.\n",
        "* FIJI always shows you xy-planes, while the bars at the bottom let you step through the c, z, or t dimensions.\n",
        "* Explore a little to get an idea of what FIJI allows you to do. In particular, the **Image > Stacks** commands have some useful functions for visualizing data, while the **Process** menu has nice functions to smooth, denoise, and so on.\n",
        "\n",
        "Try opening 'embryoVentralFurrow_myosin_membrane_live.tif' from the shared Google Drive folder in FIJI (you'll probably have to download it first). It has five dimensions, so play around with it to see how the image changes over time and depth. The image is somewhat dim, so you'll need to adjust the display settings first: go to **Image > Adjust > Brightness/Contrast**, then click 'Auto' in the box that appears. This should brighten the image some. Go to the other channel (move the bar in the image next to the letter 'c' to the right) and adjust the brighness/contrast there, too. You should now be able to see both myosin (first channel) and membranes (second channel)."
      ],
      "metadata": {
        "id": "yk7gii36FIKJ"
      },
      "id": "yk7gii36FIKJ"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}